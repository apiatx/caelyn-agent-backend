BACKEND Repl
Please make ALL of the following changes. Do not delete or modify any existing code unless I specifically say "replace."
PART 1: Add Yahoo Finance Trending Scraper
In data/finviz_scraper.py, add this standalone function at the bottom of the file (NOT inside the class):
pythonasync def scrape_yahoo_trending() -> list:
    """
    Scrape Yahoo Finance trending tickers page.
    Returns list of dicts with ticker, company name, price, change.
    This represents mainstream retail attention — different audience
    from StockTwits (active traders) or Finviz (screener users).
    """
    import httpx
    from bs4 import BeautifulSoup
    from data.cache import cache

    cache_key = "yahoo:trending"
    cached = cache.get(cache_key)
    if cached is not None:
        return cached

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        }
        async with httpx.AsyncClient() as client:
            resp = await client.get(
                "https://finance.yahoo.com/trending-tickers/",
                headers=headers,
                timeout=15,
                follow_redirects=True,
            )
        if resp.status_code != 200:
            print(f"Yahoo trending scrape failed: {resp.status_code}")
            return []

        soup = BeautifulSoup(resp.text, "html.parser")
        results = []

        # Yahoo uses table rows for trending tickers
        rows = soup.find_all("tr")
        for row in rows[:25]:
            cells = row.find_all("td")
            if len(cells) >= 5:
                ticker_link = cells[0].find("a")
                ticker = ticker_link.get_text(strip=True) if ticker_link else ""
                company = cells[1].get_text(strip=True) if len(cells) > 1 else ""
                price = cells[2].get_text(strip=True) if len(cells) > 2 else ""
                change = cells[4].get_text(strip=True) if len(cells) > 4 else ""

                if ticker and len(ticker) <= 6 and ticker.isalpha():
                    results.append({
                        "ticker": ticker.upper(),
                        "company": company,
                        "price": price,
                        "change": change,
                        "source": "yahoo_trending",
                    })

        cache.set(cache_key, results, 300)  # 5 min cache
        return results

    except Exception as e:
        print(f"Yahoo trending scrape error: {e}")
        return []
PART 2: Add StockAnalysis Trending Scraper
Add this function right below the Yahoo one:
pythonasync def scrape_stockanalysis_trending() -> list:
    """
    Scrape StockAnalysis.com most active / trending page.
    Returns list of dicts with ticker, company, volume, change.
    StockAnalysis audience = fundamental-focused retail investors.
    """
    import httpx
    from bs4 import BeautifulSoup
    from data.cache import cache

    cache_key = "stockanalysis:trending"
    cached = cache.get(cache_key)
    if cached is not None:
        return cached

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        }
        results = []

        # StockAnalysis has /markets/active/ for most active
        for page_url in [
            "https://stockanalysis.com/markets/active/",
            "https://stockanalysis.com/markets/gainers/",
        ]:
            try:
                async with httpx.AsyncClient() as client:
                    resp = await client.get(page_url, headers=headers, timeout=15, follow_redirects=True)
                if resp.status_code != 200:
                    continue

                soup = BeautifulSoup(resp.text, "html.parser")
                rows = soup.find_all("tr")
                for row in rows[:20]:
                    cells = row.find_all("td")
                    if len(cells) >= 3:
                        ticker_el = cells[0].find("a")
                        ticker = ""
                        if ticker_el:
                            # Extract ticker from link text or href
                            href = ticker_el.get("href", "")
                            text = ticker_el.get_text(strip=True)
                            if "/stocks/" in href:
                                ticker = href.split("/stocks/")[-1].strip("/").upper()
                            elif text and len(text) <= 6:
                                ticker = text.upper()

                        company = cells[1].get_text(strip=True) if len(cells) > 1 else ""
                        price = cells[2].get_text(strip=True) if len(cells) > 2 else ""
                        change = cells[3].get_text(strip=True) if len(cells) > 3 else ""
                        volume = cells[4].get_text(strip=True) if len(cells) > 4 else ""

                        if ticker and len(ticker) <= 6 and ticker.isalpha():
                            results.append({
                                "ticker": ticker,
                                "company": company,
                                "price": price,
                                "change": change,
                                "volume": volume,
                                "source": "stockanalysis",
                            })
            except Exception as inner_e:
                print(f"StockAnalysis page error ({page_url}): {inner_e}")

        # Deduplicate
        seen = set()
        deduped = []
        for r in results:
            if r["ticker"] not in seen:
                seen.add(r["ticker"])
                deduped.append(r)

        cache.set(cache_key, deduped, 300)
        return deduped

    except Exception as e:
        print(f"StockAnalysis trending scrape error: {e}")
        return []
PART 3: Add Cross-Platform Trending Aggregation Method
In data/market_data_service.py, add these imports at the top:
pythonfrom data.finviz_scraper import scrape_yahoo_trending, scrape_stockanalysis_trending
Add this method to the MarketDataService class:
python    async def get_cross_platform_trending(self) -> dict:
        """
        Aggregate trending stocks across ALL available platforms.
        Cross-references: StockTwits, Yahoo Finance, Finviz, Polygon,
        StockAnalysis. Counts how many platforms each ticker appears on.
        Stocks appearing on 3+ platforms = highest conviction trending.
        Then enriches the top tickers with full data.
        """
        import asyncio
        from data.scoring_engine import score_for_trades, passes_market_cap_filter
        from collections import Counter

        # ── Stage 1: Pull trending from ALL sources in parallel ──
        (
            stocktwits_trending,
            yahoo_trending,
            stockanalysis_trending,
            finviz_most_active,
            finviz_unusual_volume,
            finviz_top_gainers,
            polygon_movers,
        ) = await asyncio.gather(
            self.stocktwits.get_trending(),
            scrape_yahoo_trending(),
            scrape_stockanalysis_trending(),
            self.finviz.get_most_active(),
            self.finviz.get_unusual_volume(),
            self.finviz.get_screener_results("ta_topgainers"),
            asyncio.to_thread(self.polygon.get_market_movers),
            return_exceptions=True,
        )

        # Normalize errors
        if isinstance(stocktwits_trending, Exception): stocktwits_trending = []
        if isinstance(yahoo_trending, Exception): yahoo_trending = []
        if isinstance(stockanalysis_trending, Exception): stockanalysis_trending = []
        if isinstance(finviz_most_active, Exception): finviz_most_active = []
        if isinstance(finviz_unusual_volume, Exception): finviz_unusual_volume = []
        if isinstance(finviz_top_gainers, Exception): finviz_top_gainers = []
        if isinstance(polygon_movers, Exception): polygon_movers = {}

        # ── Stage 2: Extract tickers from each source with source tagging ──
        ticker_sources = {}  # {ticker: set of sources}

        def add_tickers(items, source_name, ticker_key="ticker"):
            for item in (items or []):
                if isinstance(item, dict):
                    t = item.get(ticker_key, "").upper().strip()
                    if t and len(t) <= 6 and t.isalpha():
                        if t not in ticker_sources:
                            ticker_sources[t] = set()
                        ticker_sources[t].add(source_name)

        add_tickers(stocktwits_trending, "StockTwits")
        add_tickers(yahoo_trending, "Yahoo Finance")
        add_tickers(stockanalysis_trending, "StockAnalysis")
        add_tickers(finviz_most_active, "Finviz Active")
        add_tickers(finviz_unusual_volume, "Finviz Volume")
        add_tickers(finviz_top_gainers, "Finviz Gainers")

        # Polygon movers
        for g in (polygon_movers.get("gainers") or []):
            t = g.get("ticker", "").upper()
            if t and len(t) <= 6:
                if t not in ticker_sources:
                    ticker_sources[t] = set()
                ticker_sources[t].add("Polygon")

        for l in (polygon_movers.get("losers") or []):
            t = l.get("ticker", "").upper()
            if t and len(t) <= 6:
                if t not in ticker_sources:
                    ticker_sources[t] = set()
                ticker_sources[t].add("Polygon")

        # ── Stage 3: Rank by source count ──
        ranked = sorted(
            ticker_sources.items(),
            key=lambda x: len(x[1]),
            reverse=True,
        )

        # Filter: only tickers appearing on 2+ sources
        multi_source = [(t, srcs) for t, srcs in ranked if len(srcs) >= 2]

        # Take top 20 for enrichment
        top_tickers = [t for t, _ in multi_source[:20]]

        print(f"[Trending] {len(ticker_sources)} unique tickers across all platforms")
        print(f"[Trending] {len(multi_source)} appear on 2+ platforms")
        print(f"[Trending] Top multi-platform: {[(t, len(s)) for t, s in multi_source[:10]]}")

        # ── Stage 4: Full enrichment for top tickers ──
        async def full_enrich(ticker):
            try:
                snapshot = self.polygon.get_snapshot(ticker)
                technicals = self.polygon.get_technicals(ticker)
                details = self.polygon.get_ticker_details(ticker)

                st_result, overview, analyst = await asyncio.gather(
                    self.stocktwits.get_sentiment(ticker),
                    self.stockanalysis.get_overview(ticker),
                    self.stockanalysis.get_analyst_ratings(ticker),
                    return_exceptions=True,
                )

                return {
                    "snapshot": snapshot,
                    "technicals": technicals,
                    "details": details,
                    "sentiment": st_result if not isinstance(st_result, Exception) else {},
                    "overview": overview if not isinstance(overview, Exception) else {},
                    "analyst_ratings": analyst if not isinstance(analyst, Exception) else {},
                }
            except Exception as e:
                return {"error": str(e)}

        enrichment_results = await asyncio.gather(
            *[full_enrich(t) for t in top_tickers],
            return_exceptions=True,
        )

        # ── Stage 5: Score and build final output ──
        enriched = {}
        for ticker, result in zip(top_tickers, enrichment_results):
            if isinstance(result, Exception) or not isinstance(result, dict) or "error" in result:
                continue

            # Market cap filter
            if not passes_market_cap_filter(result, "market_scan"):
                continue

            quant_score = score_for_trades(result)
            result["quant_score"] = quant_score
            result["trending_sources"] = list(ticker_sources.get(ticker, []))
            result["source_count"] = len(ticker_sources.get(ticker, []))
            enriched[ticker] = result

        # Sort by source_count first, then quant_score
        sorted_tickers = sorted(
            enriched.items(),
            key=lambda x: (x[1].get("source_count", 0), x[1].get("quant_score", 0)),
            reverse=True,
        )

        # Rebuild enriched in sorted order
        sorted_enriched = {}
        for t, d in sorted_tickers:
            sorted_enriched[t] = d

        return {
            "total_unique_tickers": len(ticker_sources),
            "multi_platform_count": len(multi_source),
            "source_summary": {
                "StockTwits": len(stocktwits_trending),
                "Yahoo Finance": len(yahoo_trending),
                "StockAnalysis": len(stockanalysis_trending),
                "Finviz Active": len(finviz_most_active),
                "Finviz Volume": len(finviz_unusual_volume),
                "Finviz Gainers": len(finviz_top_gainers),
                "Polygon": len(polygon_movers.get("gainers", [])) + len(polygon_movers.get("losers", [])),
            },
            "ranked_tickers": [
                {
                    "ticker": t,
                    "source_count": d.get("source_count", 0),
                    "sources": d.get("trending_sources", []),
                    "quant_score": d.get("quant_score", 0),
                }
                for t, d in sorted_tickers[:15]
            ],
            "enriched_data": sorted_enriched,
            "market_news": self.polygon.get_news(limit=10),
        }
PART 4: Add Trending Route in Agent
In agent/claude_agent.py, add to _gather_data:
python        elif category == "trending":
            return await self.data.get_cross_platform_trending()
```

### PART 5: Add Trending to Classifier

**In `agent/prompts.py`, in `QUERY_CLASSIFIER_PROMPT`, add:**
```
- "trending": User asks what's trending, what's hot, what everyone is watching, popular stocks right now, most mentioned stocks, what's getting attention, viral stocks. Cross-references multiple platforms for trending data.
```

### PART 6: Add Trending Display Format to System Prompt

**In `agent/prompts.py`, add this format before `### FORMAT 7: "chat"`:**
```
### FORMAT: "trending" — Cross-Platform Trending Aggregation
Use when: user asks what's trending, what's hot, what everyone is watching.

The data you receive has been aggregated across 7 sources:
- StockTwits (active trader attention)
- Yahoo Finance (mainstream retail attention)
- StockAnalysis (fundamental investor attention)
- Finviz Most Active (highest trading volume)
- Finviz Unusual Volume (volume spikes vs average)
- Finviz Top Gainers (biggest price movers)
- Polygon (market-wide gainers/losers)

Each ticker has a `source_count` — the number of platforms it appears on simultaneously.
This is the key metric: 5+ sources = maximum conviction trending. 3-4 = strong. 2 = moderate.

A stock appearing on StockTwits + Yahoo + Finviz Active + Polygon = traders AND mainstream AND volume all aligned.
```json
{
  "display_type": "trending",
  "summary": "Scanned 187 unique tickers across 7 platforms. 23 appear on 2+ platforms. NVDA leads with 6/7 sources — everyone is watching AI.",
  "source_coverage": {
    "StockTwits": 30,
    "Yahoo Finance": 15,
    "StockAnalysis": 20,
    "Finviz Active": 20,
    "Finviz Volume": 20,
    "Finviz Gainers": 20,
    "Polygon": 40
  },
  "trending_tickers": [
    {
      "ticker": "NVDA",
      "company": "NVIDIA Corporation",
      "source_count": 6,
      "sources": ["StockTwits", "Yahoo Finance", "StockAnalysis", "Finviz Active", "Finviz Volume", "Polygon"],
      "price": "$875",
      "change": "+3.2%",
      "volume_vs_avg": "2.8x",
      "market_cap": "$2.1T",
      "quant_score": 78,
      "why_trending": "AI infrastructure demand accelerating. Earnings beat + raised guidance. Every platform watching. Volume 2.8x average confirms institutional participation.",
      "sentiment": "78% bullish on StockTwits",
      "ta_summary": "RSI 62 | Above all SMAs | MACD bullish",
      "fundamental_snapshot": "Rev +94% YoY | EBITDA 65% | P/E 45x",
      "analyst_consensus": "42 Buy, 5 Hold, 1 Sell",
      "verdict": "Trending for good reason. AI theme intact. Setup still healthy (RSI not overbought). The consensus is bullish but the technicals confirm it.",
      "risk": "Already up 180% in 12 months. Any AI narrative shift = sharp pullback.",
      "conviction": "High"
    }
  ],
  "platform_divergences": [
    {"observation": "SMCI trending on StockTwits + Finviz but NOT on Yahoo — trader-driven momentum, not mainstream yet. Could be early."},
    {"observation": "TSLA on Yahoo + Polygon but NOT StockTwits — mainstream curiosity without trader conviction. Be cautious."}
  ]
}
```

RULES FOR TRENDING FORMAT:
- Sort by source_count descending. The MORE platforms a stock appears on, the higher it should rank.
- Every ticker MUST show: which specific platforms it's trending on, why it's trending, sentiment, TA summary, fundamental snapshot
- Include StockAnalysis data where available: revenue growth, margins, analyst ratings, valuation
- Include platform_divergences: when a stock is trending on some platforms but not others, explain what that means
  - StockTwits only = trader-driven, speculative
  - Yahoo only = mainstream/retail curiosity
  - Finviz Volume only = institutional activity without retail attention (potentially early signal)
  - All platforms = maximum consensus (strongest signal but also watch for being too crowded)
- Flag any ticker where source_count is 4+ as "Maximum Attention"
- Flag any ticker trending on ALL platforms where RSI > 70 as "Overbought + Maximum Attention = Potential Top"
- Include why_trending for each — this is not just a list, explain the narrative driving attention
- Use StockAnalysis fundamentals (revenue growth, margins, P/E) to distinguish between trending because of FUNDAMENTALS vs trending because of HYPE
Re-publish/deploy the backend.