The project files I have access to are the pre-implementation snapshot. The Replit agent's changes aren't reflected in my project files. That's fine — based on the agent's summary, Phase 1 is done:

Finviz-first price extraction in _enrich_one
api_budget.py with daily tracking
Budget wired into candle fetching, quote batching, macro snapshots, screener enrichment, ticker research
/api/health/budget endpoint
All 54 tests passing

Phase 1 is complete. No corrections needed — the agent's implementation log shows the right pattern (check → call → spend only on success).

Phase 2: Data Compression Layer — Pre-Digest Before Claude
Here are the exact instructions to give Replit Agent:

Prompt for Replit Agent:
## Task: Build a Data Compression Layer that pre-digests raw market data before sending it to Claude

### Problem
Right now, Claude receives the full raw `market_data` dict (often 50-150KB of JSON) and has to simultaneously parse it, identify what matters, apply scoring, form opinions, and format output. This wastes context window on raw data and degrades reasoning quality.

### Solution
Expand the existing `data_compressor.py` into a proper pre-digest layer that runs AFTER data gathering and scoring but BEFORE Claude. It should compress the raw data blob into a concise structured summary that highlights what actually matters.

### Architecture

The compression layer sits in the pipeline at this point:
Data gathering → Scoring/Regime → DATA COMPRESSION → Claude narration

This means it should be called in `claude_agent.py` inside `handle_query()`, right before `_ask_claude_with_timeout()`.

### Implementation Details

#### 1. Expand `data_compressor.py` with these functions:
```python
"""
Data Compression Layer — pre-digests raw market data before Claude.

Converts 50-150KB raw data blobs into 5-15KB structured summaries
that highlight actionable signals and eliminate noise.

Runs AFTER scoring, BEFORE Claude.
"""

def compress_for_claude(market_data: dict, category: str) -> dict:
    """
    Main entry point. Routes to category-specific compressor.
    Returns a compressed dict that replaces market_data in the Claude prompt.
    
    The compressed output ALWAYS includes:
    - _raw_size: original data size in chars (for logging)
    - _compressed_size: compressed size in chars
    - _compression_ratio: how much smaller
    """
    if not market_data or not isinstance(market_data, dict):
        return market_data
    
    import json
    raw_size = len(json.dumps(market_data, default=str))
    
    compressors = {
        "best_trades": _compress_best_trades,
        "briefing": _compress_briefing,
        "cross_asset_trending": _compress_trending,
        "trending": _compress_trending,
        "cross_market": _compress_trending,
        "deterministic_screener": _compress_screener,
        "crypto": _compress_crypto,
        "sector_rotation": _compress_sector,
        "macro_outlook": _compress_macro,
    }
    
    compressor = compressors.get(category)
    if compressor:
        compressed = compressor(market_data)
    else:
        # For unknown categories, do light compression (strip nulls, trim lists)
        compressed = _compress_generic(market_data)
    
    compressed_size = len(json.dumps(compressed, default=str))
    compressed["_compression"] = {
        "raw_size": raw_size,
        "compressed_size": compressed_size,
        "ratio": round(raw_size / max(compressed_size, 1), 1),
        "category": category,
    }
    
    return compressed


def _compress_best_trades(data: dict) -> dict:
    """
    Best Trades: Claude needs the trade plans + enough context to form opinions.
    Keep: top_trades with all TA fields, market_pulse, scan_stats, data_health
    Strip: raw candle data, redundant finviz fields, internal debug
    Add: pre-digested summary of what the TA engine found
    """
    top_trades = data.get("top_trades", [])
    bearish = data.get("bearish_setups", [])
    
    # Build a TA summary digest Claude can reason about quickly
    trade_digest = []
    for t in top_trades[:10]:
        digest = {
            "ticker": t.get("ticker"),
            "price": t.get("price"),
            "technical_score": t.get("technical_score"),
            "confidence_score": t.get("confidence_score"),
            "pattern": t.get("pattern"),
            "setup_type": t.get("setup_type"),
            "signals_stacking": t.get("signals_stacking", []),
            "indicator_signals": t.get("indicator_signals", []),
            "trade_plan": {
                "entry": t.get("entry"),
                "stop": t.get("stop"),
                "targets": t.get("targets"),
                "risk_reward": t.get("risk_reward"),
                "timeframe": t.get("timeframe"),
            },
            "volume_confirmation": t.get("volume_confirmation"),
            "market_cap": t.get("market_cap"),
            "name": t.get("name"),
            "sector": t.get("sector"),
            # Fundamental context if enriched
            "pe_ratio": t.get("pe_ratio"),
            "exchange": t.get("exchange"),
            "tradingview_url": t.get("tradingview_url"),
            # EDGAR data if present
            "edgar": t.get("edgar"),
            # Source attribution
            "source_screens": t.get("source_screens", []),
        }
        # Remove None values to save tokens
        digest = {k: v for k, v in digest.items() if v is not None}
        trade_digest.append(digest)
    
    bearish_digest = []
    for t in bearish[:3]:
        digest = {
            "ticker": t.get("ticker"),
            "price": t.get("price"),
            "technical_score": t.get("technical_score"),
            "confidence_score": t.get("confidence_score"),
            "pattern": t.get("pattern"),
            "signals_stacking": t.get("signals_stacking", []),
            "trade_plan": {
                "entry": t.get("entry"),
                "stop": t.get("stop"),
                "targets": t.get("targets"),
                "risk_reward": t.get("risk_reward"),
            },
        }
        digest = {k: v for k, v in digest.items() if v is not None}
        bearish_digest.append(digest)
    
    # Pre-digest: what patterns dominated?
    pattern_counts = {}
    for t in top_trades:
        p = t.get("pattern", "unknown")
        pattern_counts[p] = pattern_counts.get(p, 0) + 1
    
    avg_score = round(sum(t.get("technical_score", 0) for t in top_trades) / max(len(top_trades), 1), 1)
    avg_rr = []
    for t in top_trades:
        rr = t.get("risk_reward")
        if rr:
            try:
                avg_rr.append(float(str(rr).replace(":", "").replace("R", "").strip().split()[0]) if ":" not in str(rr) else float(str(rr).split(":")[1]))
            except:
                pass
    
    return {
        "scan_type": "best_trades",
        "display_type": "trades",
        "digest": {
            "total_candidates_scanned": data.get("scan_stats", {}).get("candidates_found", 0),
            "ta_qualified": data.get("scan_stats", {}).get("ta_qualified", 0),
            "avg_technical_score": avg_score,
            "dominant_patterns": pattern_counts,
            "avg_risk_reward": round(sum(avg_rr) / max(len(avg_rr), 1), 2) if avg_rr else None,
        },
        "market_pulse": data.get("market_pulse", {}),
        "top_trades": trade_digest,
        "bearish_setups": bearish_digest,
        "scan_stats": data.get("scan_stats", {}),
        "data_health": data.get("data_health", {}),
    }


def _compress_briefing(data: dict) -> dict:
    """
    Daily Briefing: Claude needs macro snapshot, signal highlights, top movers.
    Strip: raw enriched_data blobs, full screener results
    Keep: pre_computed_highlights, macro_snapshot, ranked_candidates, fear_greed
    """
    highlights = data.get("pre_computed_highlights", data.get("highlights", {}))
    
    # Compress ranked candidates to essential fields only
    ranked = []
    for c in data.get("ranked_candidates", [])[:15]:
        entry = {
            "ticker": c.get("ticker"),
            "trade_score": c.get("trade_score"),
            "invest_score": c.get("invest_score"),
            "signal_count": c.get("signal_count"),
            "signal_sources": c.get("signal_sources", []),
        }
        entry = {k: v for k, v in entry.items() if v is not None}
        ranked.append(entry)
    
    # Compress enriched data — only keep sentiment and key overview fields
    enriched_compact = {}
    for ticker, d in data.get("enriched_data", {}).items():
        compact = {}
        sentiment = d.get("sentiment", {})
        if sentiment and isinstance(sentiment, dict):
            compact["sentiment"] = sentiment.get("sentiment", "unknown")
            compact["bull_pct"] = sentiment.get("bullish_pct")
        overview = d.get("overview", {})
        if overview and isinstance(overview, dict):
            for key in ("market_cap", "pe_ratio", "revenue_growth", "sector"):
                if overview.get(key):
                    compact[key] = overview[key]
        if compact:
            enriched_compact[ticker] = compact
    
    return {
        "scan_type": "briefing",
        "display_type": "briefing",
        "pre_computed_highlights": highlights,
        "macro_snapshot": data.get("macro_snapshot", {}),
        "news_context": _trim_news(data.get("news_context", {})),
        "total_tickers_detected": data.get("total_tickers_detected", 0),
        "multi_signal_tickers": data.get("multi_signal_tickers", {}),
        "ranked_candidates": ranked,
        "enriched_compact": enriched_compact,
        "fear_greed": data.get("fear_greed", {}),
        "fred_macro": data.get("fred_macro", {}),
        "highlights": highlights,
        "upcoming_earnings": data.get("upcoming_earnings", []),
    }


def _compress_trending(data: dict) -> dict:
    """
    Trending/Cross-Asset: Keep ranked picks with scoring, trim raw provider data.
    The cross_asset_ranker already does good work — preserve its output.
    """
    # If this is cross-asset ranker output, it's already reasonably structured
    if "picks" in data or "trending_tickers" in data:
        # Strip debug/internal fields
        compressed = {k: v for k, v in data.items() 
                     if not k.startswith("_") and k not in ("raw_stock_data", "raw_crypto_data", "raw_commodity_data")}
        
        # Trim any embedded x_social_scan to essentials
        x_data = compressed.get("x_social_scan", {})
        if isinstance(x_data, dict) and "trending_tickers" in x_data:
            trimmed_tickers = []
            for t in x_data.get("trending_tickers", [])[:15]:
                trimmed = {
                    "ticker": t.get("ticker"),
                    "sentiment": t.get("sentiment"),
                    "sentiment_score": t.get("sentiment_score"),
                    "mention_intensity": t.get("mention_intensity"),
                    "why_trending": t.get("why_trending"),
                    "catalyst": t.get("catalyst"),
                    "risk_flag": t.get("risk_flag"),
                }
                trimmed = {k: v for k, v in trimmed.items() if v is not None}
                trimmed_tickers.append(trimmed)
            x_data["trending_tickers"] = trimmed_tickers
            # Remove verbose fields
            for key in ("sector_heat", "contrarian_signals"):
                x_data.pop(key, None)
        
        return compressed
    
    return _compress_generic(data)


def _compress_screener(data: dict) -> dict:
    """
    Screener results: Already well-structured from run_deterministic_screener.
    Strip: internal _ta dicts, _sma50_trending_up flags, missing_fields details.
    Keep: rows with signals, top_picks, explain, scan_stats.
    """
    rows = []
    for row in data.get("rows", []):
        clean = {k: v for k, v in row.items() 
                if not k.startswith("_") and v is not None}
        # Remove internal scoring fields user doesn't need
        clean.pop("missing_fields", None)
        rows.append(clean)
    
    return {
        "display_type": "screener",
        "screen_name": data.get("screen_name", ""),
        "preset": data.get("preset", ""),
        "explain": data.get("explain", []),
        "top_picks": data.get("top_picks", []),
        "rows": rows,
        "scan_stats": data.get("scan_stats", {}),
        "meta": data.get("meta", {}),
    }


def _compress_crypto(data: dict) -> dict:
    """Crypto scans: trim raw coingecko/cmc data, keep structured analysis."""
    return _compress_generic(data)


def _compress_sector(data: dict) -> dict:
    """Sector rotation: trim raw ETF data, keep rotation signals."""
    return _compress_generic(data)


def _compress_macro(data: dict) -> dict:
    """Macro outlook: keep FRED data, trim redundant news."""
    compressed = dict(data)
    compressed["news_context"] = _trim_news(data.get("news_context", {}))
    return compressed


def _compress_generic(data: dict) -> dict:
    """
    Generic compression for unknown categories:
    - Remove None values recursively
    - Trim lists to reasonable lengths
    - Strip internal debug fields (keys starting with _)
    """
    def _clean(obj, depth=0):
        if depth > 5:
            return obj
        if isinstance(obj, dict):
            return {k: _clean(v, depth+1) for k, v in obj.items() 
                    if v is not None and not (isinstance(k, str) and k.startswith("_") and k != "_routing")}
        if isinstance(obj, list):
            # Trim long lists
            items = obj[:30] if len(obj) > 30 else obj
            return [_clean(item, depth+1) for item in items if item is not None]
        return obj
    
    return _clean(data)


def _trim_news(news_context: dict) -> dict:
    """Trim news articles to headlines + sentiment only."""
    if not isinstance(news_context, dict):
        return news_context
    
    trimmed = {}
    for key, articles in news_context.items():
        if isinstance(articles, list):
            trimmed[key] = []
            for a in articles[:8]:
                if isinstance(a, dict):
                    trimmed[key].append({
                        "title": a.get("title", a.get("headline", "")),
                        "source": a.get("source", ""),
                        "sentiment": a.get("overall_sentiment_label", a.get("sentiment", "")),
                        "tickers": a.get("ticker_sentiment", a.get("tickers", [])),
                    })
                else:
                    trimmed[key].append(a)
        else:
            trimmed[key] = articles
    
    return trimmed
```

#### 2. Wire compression into `claude_agent.py`

In the `handle_query()` method, add compression right before the Claude call. Find this section (around line 401-404):
```python
        data_done_time = time.time()
        data_ms = int((data_done_time - start_time) * 1000)

        raw_response = await self._ask_claude_with_timeout(user_prompt, market_data, history, is_followup=is_followup, category=category)
```

Replace it with:
```python
        data_done_time = time.time()
        data_ms = int((data_done_time - start_time) * 1000)

        # Compress data before sending to Claude — reduces context window usage
        claude_data = market_data
        if market_data and isinstance(market_data, dict) and category != "followup":
            try:
                from data_compressor import compress_for_claude
                claude_data = compress_for_claude(market_data, category)
                compression = claude_data.get("_compression", {})
                print(f"[COMPRESS] {compression.get('raw_size', 0):,} → {compression.get('compressed_size', 0):,} chars "
                      f"({compression.get('ratio', 1)}x reduction) for category={category}")
            except Exception as e:
                print(f"[COMPRESS] Compression failed, using raw data: {e}")
                claude_data = market_data

        raw_response = await self._ask_claude_with_timeout(user_prompt, claude_data, history, is_followup=is_followup, category=category)
```

IMPORTANT: The uncompressed `market_data` must still be available after Claude responds, because the post-processing logic (lines 412-544) reads fields like `top_trades`, `rows`, `top_picks`, `data_health` etc. from the original market_data. So do NOT replace `market_data` — create a new variable `claude_data` for what goes to Claude, and keep `market_data` intact for post-processing.

#### 3. Keep the existing data_compressor.py content

Check if data_compressor.py already has content. If it does, preserve any existing functions and add the new ones. If it's mostly empty or just has stubs, replace it entirely with the code above.

### Testing Checklist

After implementing, test each preset:

1. `/briefing` — Check logs for `[COMPRESS]` line showing ratio. Should see 2-5x compression.
2. `/trades` (Best Trades) — Verify top_trades still render correctly in terminal. Check compression ratio.
3. All 6 screener presets — Verify rows, top_picks, explain all still populate. Screener output should be unchanged.
4. `Trending Now` — Verify trending_tickers array still populates with ratings/confidence.
5. Run existing test suite: `python -m pytest` — all tests must still pass.

### Success Criteria

- `[COMPRESS]` log line appears on every scan request
- Compression ratio is 2x+ for Best Trades, Briefing, and Trending
- All frontend rendering unchanged — no schema changes, no missing fields
- Claude's narration quality should be equal or better (less noise in context = better reasoning)
- All existing tests pass

### Non-negotiables

- Do NOT change any response schemas or frontend contracts
- Do NOT remove fields from the final response that goes to the frontend — compression only affects what Claude sees
- Keep `market_data` intact for post-processing after Claude responds
- If compression fails for any reason, fall back to uncompressed data silently

